{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#define the modified AlexNet\n",
        "class AlexNetModified(nn.Module):\n",
        "    def __init__(self, num_classes=10, use_dropout=False):\n",
        "        super(AlexNetModified, self).__init__()\n",
        "        self.use_dropout = use_dropout\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5) if use_dropout else nn.Identity(),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5) if use_dropout else nn.Identity(),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "##############Function to count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "#############Load CIFAR-10 & CIFAR-100 datasets\n",
        "trainloader_10, testloader_10, trainloader_100, testloader_100 = None, None, None, None\n",
        "\n",
        "def get_data_loaders(batch_size=128):\n",
        "    global trainloader_10, testloader_10, trainloader_100, testloader_100\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    trainset_10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    testset_10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    trainset_100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    testset_100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    trainloader_10, testloader_10 = DataLoader(trainset_10, batch_size=batch_size, shuffle=True), DataLoader(testset_10, batch_size=batch_size, shuffle=False)\n",
        "    trainloader_100, testloader_100 = DataLoader(trainset_100, batch_size=batch_size, shuffle=True), DataLoader(testset_100, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#######Initialize data loaders\n",
        "get_data_loaders()\n",
        "\n",
        "#####Train function\n",
        "def train_model(model, trainloader, testloader, num_epochs=10, learning_rate=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "\n",
        "        model.eval()\n",
        "        correct, total, val_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_losses.append(val_loss / len(testloader))\n",
        "        val_accuracies.append(correct / total * 100)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "######Train CIFAR-10 models\n",
        "print(\"Training AlexNet Without Dropout on CIFAR-10\")\n",
        "train_losses_no_dropout_10, val_losses_no_dropout_10, val_acc_no_dropout_10 = train_model(AlexNetModified(num_classes=10, use_dropout=False), trainloader_10, testloader_10)\n",
        "print(\"Training AlexNet With Dropout on CIFAR-10\")\n",
        "train_losses_dropout_10, val_losses_dropout_10, val_acc_dropout_10 = train_model(AlexNetModified(num_classes=10, use_dropout=True), trainloader_10, testloader_10)\n",
        "\n",
        "######Train CIFAR-100 models\n",
        "print(\"Training AlexNet Without Dropout on CIFAR-100\")\n",
        "train_losses_no_dropout_100, val_losses_no_dropout_100, val_acc_no_dropout_100 = train_model(AlexNetModified(num_classes=100, use_dropout=False), trainloader_100, testloader_100)\n",
        "print(\"Training AlexNet With Dropout on CIFAR-100\")\n",
        "train_losses_dropout_100, val_losses_dropout_100, val_acc_dropout_100 = train_model(AlexNetModified(num_classes=100, use_dropout=True), trainloader_100, testloader_100)\n",
        "\n",
        "### Results table\n",
        "results = pd.DataFrame({\n",
        "    \"Dataset\": [\"CIFAR-10\", \"CIFAR-10\", \"CIFAR-100\", \"CIFAR-100\"],\n",
        "    \"Dropout\": [\"No\", \"Yes\", \"No\", \"Yes\"],\n",
        "    \"Final Val Accuracy (%)\": [val_acc_no_dropout_10[-1], val_acc_dropout_10[-1], val_acc_no_dropout_100[-1], val_acc_dropout_100[-1]],\n",
        "    \"Final Val Loss\": [val_losses_no_dropout_10[-1], val_losses_dropout_10[-1], val_losses_no_dropout_100[-1], val_losses_dropout_100[-1]]\n",
        "})\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00l8Ae6JAw0a",
        "outputId": "b08775a5-fb47-4f81-d49a-4f0e07ac40b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training AlexNet Without Dropout on CIFAR-10\n",
            "Epoch 1/10 - Train Loss: 1.6413, Val Loss: 1.3055, Val Acc: 51.94%\n",
            "Epoch 2/10 - Train Loss: 1.1862, Val Loss: 1.0719, Val Acc: 61.84%\n",
            "Epoch 3/10 - Train Loss: 0.9689, Val Loss: 0.8917, Val Acc: 68.43%\n",
            "Epoch 4/10 - Train Loss: 0.8328, Val Loss: 0.8472, Val Acc: 70.54%\n",
            "Epoch 5/10 - Train Loss: 0.7378, Val Loss: 0.7374, Val Acc: 74.98%\n",
            "Epoch 6/10 - Train Loss: 0.6713, Val Loss: 0.7071, Val Acc: 75.51%\n",
            "Epoch 7/10 - Train Loss: 0.6151, Val Loss: 0.6742, Val Acc: 76.75%\n",
            "Epoch 8/10 - Train Loss: 0.5691, Val Loss: 0.6194, Val Acc: 78.18%\n",
            "Epoch 9/10 - Train Loss: 0.5324, Val Loss: 0.6050, Val Acc: 79.50%\n",
            "Epoch 10/10 - Train Loss: 0.5026, Val Loss: 0.6089, Val Acc: 79.52%\n",
            "Training AlexNet With Dropout on CIFAR-10\n",
            "Epoch 1/10 - Train Loss: 1.7561, Val Loss: 1.4997, Val Acc: 44.55%\n",
            "Epoch 2/10 - Train Loss: 1.3445, Val Loss: 1.2412, Val Acc: 54.64%\n",
            "Epoch 3/10 - Train Loss: 1.1572, Val Loss: 1.0609, Val Acc: 62.29%\n",
            "Epoch 4/10 - Train Loss: 1.0298, Val Loss: 0.9796, Val Acc: 65.27%\n",
            "Epoch 5/10 - Train Loss: 0.9434, Val Loss: 0.8679, Val Acc: 69.69%\n",
            "Epoch 6/10 - Train Loss: 0.8652, Val Loss: 0.8270, Val Acc: 70.71%\n",
            "Epoch 7/10 - Train Loss: 0.8110, Val Loss: 0.7715, Val Acc: 73.10%\n",
            "Epoch 8/10 - Train Loss: 0.7709, Val Loss: 0.7773, Val Acc: 73.18%\n",
            "Epoch 9/10 - Train Loss: 0.7346, Val Loss: 0.7199, Val Acc: 75.05%\n",
            "Epoch 10/10 - Train Loss: 0.7037, Val Loss: 0.6995, Val Acc: 75.68%\n",
            "Training AlexNet Without Dropout on CIFAR-100\n",
            "Epoch 1/10 - Train Loss: 4.0958, Val Loss: 3.7524, Val Acc: 11.46%\n",
            "Epoch 2/10 - Train Loss: 3.5127, Val Loss: 3.2839, Val Acc: 19.52%\n",
            "Epoch 3/10 - Train Loss: 3.1136, Val Loss: 3.0828, Val Acc: 24.17%\n",
            "Epoch 4/10 - Train Loss: 2.8401, Val Loss: 2.7500, Val Acc: 30.28%\n",
            "Epoch 5/10 - Train Loss: 2.6284, Val Loss: 2.6108, Val Acc: 33.16%\n",
            "Epoch 6/10 - Train Loss: 2.4472, Val Loss: 2.4923, Val Acc: 35.70%\n",
            "Epoch 7/10 - Train Loss: 2.3193, Val Loss: 2.3835, Val Acc: 37.65%\n",
            "Epoch 8/10 - Train Loss: 2.1996, Val Loss: 2.3961, Val Acc: 37.85%\n",
            "Epoch 9/10 - Train Loss: 2.0952, Val Loss: 2.2547, Val Acc: 40.85%\n",
            "Epoch 10/10 - Train Loss: 2.0134, Val Loss: 2.1943, Val Acc: 42.31%\n",
            "Training AlexNet With Dropout on CIFAR-100\n",
            "Epoch 1/10 - Train Loss: 4.2744, Val Loss: 3.8942, Val Acc: 9.15%\n",
            "Epoch 2/10 - Train Loss: 3.7774, Val Loss: 3.5377, Val Acc: 15.59%\n",
            "Epoch 3/10 - Train Loss: 3.5148, Val Loss: 3.2668, Val Acc: 20.35%\n",
            "Epoch 4/10 - Train Loss: 3.2981, Val Loss: 3.0919, Val Acc: 23.78%\n",
            "Epoch 5/10 - Train Loss: 3.1392, Val Loss: 2.9387, Val Acc: 26.08%\n",
            "Epoch 6/10 - Train Loss: 3.0058, Val Loss: 2.8941, Val Acc: 27.21%\n",
            "Epoch 7/10 - Train Loss: 2.8997, Val Loss: 2.7233, Val Acc: 30.78%\n",
            "Epoch 8/10 - Train Loss: 2.8041, Val Loss: 2.6072, Val Acc: 33.38%\n",
            "Epoch 9/10 - Train Loss: 2.7092, Val Loss: 2.5672, Val Acc: 34.80%\n",
            "Epoch 10/10 - Train Loss: 2.6405, Val Loss: 2.5369, Val Acc: 34.98%\n",
            "     Dataset Dropout  Final Val Accuracy (%)  Final Val Loss\n",
            "0   CIFAR-10      No                   79.52        0.608917\n",
            "1   CIFAR-10     Yes                   75.68        0.699549\n",
            "2  CIFAR-100      No                   42.31        2.194348\n",
            "3  CIFAR-100     Yes                   34.98        2.536894\n"
          ]
        }
      ]
    }
  ]
}