{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5fCa3_EolED",
        "outputId": "14a677d5-572f-49ad-da17-06b1b429469f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original VGG-16 Parameters: 138000000\n",
            "Modified VGG Parameters: 5870666\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 57.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:01<00:00, 105MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training VGGNet Without Dropout on CIFAR-10\n",
            "Epoch 1/10 - Train Loss: 1.7151, Val Loss: 1.3681, Val Acc: 49.89%\n",
            "Epoch 2/10 - Train Loss: 1.1900, Val Loss: 1.0580, Val Acc: 62.40%\n",
            "Epoch 3/10 - Train Loss: 0.9527, Val Loss: 0.8564, Val Acc: 69.72%\n",
            "Epoch 4/10 - Train Loss: 0.8005, Val Loss: 0.7886, Val Acc: 72.57%\n",
            "Epoch 5/10 - Train Loss: 0.6916, Val Loss: 0.6864, Val Acc: 76.60%\n",
            "Epoch 6/10 - Train Loss: 0.6191, Val Loss: 0.6139, Val Acc: 78.96%\n",
            "Epoch 7/10 - Train Loss: 0.5599, Val Loss: 0.6174, Val Acc: 79.07%\n",
            "Epoch 8/10 - Train Loss: 0.5136, Val Loss: 0.5721, Val Acc: 80.93%\n",
            "Epoch 9/10 - Train Loss: 0.4822, Val Loss: 0.5900, Val Acc: 80.42%\n",
            "Epoch 10/10 - Train Loss: 0.4523, Val Loss: 0.5311, Val Acc: 81.93%\n",
            "Training VGGNet With Dropout on CIFAR-10\n",
            "Epoch 1/10 - Train Loss: 1.8088, Val Loss: 1.5187, Val Acc: 42.10%\n",
            "Epoch 2/10 - Train Loss: 1.3316, Val Loss: 1.1319, Val Acc: 59.12%\n",
            "Epoch 3/10 - Train Loss: 1.0958, Val Loss: 0.9875, Val Acc: 64.34%\n",
            "Epoch 4/10 - Train Loss: 0.9508, Val Loss: 0.8730, Val Acc: 68.97%\n",
            "Epoch 5/10 - Train Loss: 0.8414, Val Loss: 0.7698, Val Acc: 73.36%\n",
            "Epoch 6/10 - Train Loss: 0.7568, Val Loss: 0.7273, Val Acc: 74.65%\n",
            "Epoch 7/10 - Train Loss: 0.7071, Val Loss: 0.7042, Val Acc: 75.88%\n",
            "Epoch 8/10 - Train Loss: 0.6548, Val Loss: 0.6531, Val Acc: 77.59%\n",
            "Epoch 9/10 - Train Loss: 0.6236, Val Loss: 0.6700, Val Acc: 77.37%\n",
            "Epoch 10/10 - Train Loss: 0.5815, Val Loss: 0.6080, Val Acc: 78.94%\n",
            "Training VGGNet Without Dropout on CIFAR-100\n",
            "Epoch 1/10 - Train Loss: 4.2329, Val Loss: 3.8627, Val Acc: 9.07%\n",
            "Epoch 2/10 - Train Loss: 3.6291, Val Loss: 3.4965, Val Acc: 16.26%\n",
            "Epoch 3/10 - Train Loss: 3.2939, Val Loss: 3.2379, Val Acc: 20.24%\n",
            "Epoch 4/10 - Train Loss: 3.0527, Val Loss: 2.9814, Val Acc: 26.38%\n",
            "Epoch 5/10 - Train Loss: 2.8313, Val Loss: 2.7736, Val Acc: 30.21%\n",
            "Epoch 6/10 - Train Loss: 2.6331, Val Loss: 2.6237, Val Acc: 33.01%\n",
            "Epoch 7/10 - Train Loss: 2.4716, Val Loss: 2.4837, Val Acc: 36.18%\n",
            "Epoch 8/10 - Train Loss: 2.3285, Val Loss: 2.4722, Val Acc: 36.13%\n",
            "Epoch 9/10 - Train Loss: 2.2168, Val Loss: 2.3388, Val Acc: 39.41%\n",
            "Epoch 10/10 - Train Loss: 2.1051, Val Loss: 2.2581, Val Acc: 40.47%\n",
            "Training VGGNet With Dropout on CIFAR-100\n",
            "Epoch 1/10 - Train Loss: 4.2978, Val Loss: 3.9754, Val Acc: 6.31%\n",
            "Epoch 2/10 - Train Loss: 3.8819, Val Loss: 3.6816, Val Acc: 12.11%\n",
            "Epoch 3/10 - Train Loss: 3.6336, Val Loss: 3.3902, Val Acc: 18.01%\n",
            "Epoch 4/10 - Train Loss: 3.4336, Val Loss: 3.2219, Val Acc: 21.60%\n",
            "Epoch 5/10 - Train Loss: 3.2768, Val Loss: 3.0582, Val Acc: 24.35%\n",
            "Epoch 6/10 - Train Loss: 3.1554, Val Loss: 2.9583, Val Acc: 25.86%\n",
            "Epoch 7/10 - Train Loss: 3.0507, Val Loss: 2.9039, Val Acc: 27.97%\n",
            "Epoch 8/10 - Train Loss: 2.9435, Val Loss: 2.7702, Val Acc: 30.69%\n",
            "Epoch 9/10 - Train Loss: 2.8617, Val Loss: 2.6794, Val Acc: 32.50%\n",
            "Epoch 10/10 - Train Loss: 2.7850, Val Loss: 2.6029, Val Acc: 33.59%\n",
            "     Dataset Dropout  Final Val Accuracy (%)  Final Val Loss\n",
            "0   CIFAR-10      No                   81.93        0.531077\n",
            "1   CIFAR-10     Yes                   78.94        0.607952\n",
            "2  CIFAR-100      No                   40.47        2.258094\n",
            "3  CIFAR-100     Yes                   33.59        2.602877\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define the modified VGGNet\n",
        "class VGGModified(nn.Module):\n",
        "    def __init__(self, num_classes=10, use_dropout=False):\n",
        "        super(VGGModified, self).__init__()\n",
        "        self.use_dropout = use_dropout\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5) if use_dropout else nn.Identity(),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5) if use_dropout else nn.Identity(),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Function to count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "##############3Compare parameter counts\n",
        "original_vgg16_params = 138000000  #\n",
        "modified_vgg_params = count_parameters(VGGModified(num_classes=10, use_dropout=False))\n",
        "print(f\"Original VGG-16 Parameters: {original_vgg16_params}\")\n",
        "print(f\"Modified VGG Parameters: {modified_vgg_params}\")\n",
        "\n",
        "#######Load CIFAR-10 & CIFAR-100 datasets\n",
        "def get_data_loaders(batch_size=128):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    trainset_10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    testset_10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    trainset_100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    testset_100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    return DataLoader(trainset_10, batch_size=batch_size, shuffle=True), DataLoader(testset_10, batch_size=batch_size, shuffle=False), DataLoader(trainset_100, batch_size=batch_size, shuffle=True), DataLoader(testset_100, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "##### Training & evaluation function\n",
        "def train_model(model, trainloader, testloader, num_epochs=10, learning_rate=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "\n",
        "        model.eval()\n",
        "        correct, total, val_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_losses.append(val_loss / len(testloader))\n",
        "        val_accuracies.append(correct / total * 100)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "#Train and compare results\n",
        "trainloader_10, testloader_10, trainloader_100, testloader_100 = get_data_loaders()\n",
        "print(\"Training VGGNet Without Dropout on CIFAR-10\")\n",
        "train_losses_no_dropout, val_losses_no_dropout, val_acc_no_dropout = train_model(VGGModified(num_classes=10, use_dropout=False), trainloader_10, testloader_10)\n",
        "print(\"Training VGGNet With Dropout on CIFAR-10\")\n",
        "train_losses_dropout, val_losses_dropout, val_acc_dropout = train_model(VGGModified(num_classes=10, use_dropout=True), trainloader_10, testloader_10)\n",
        "\n",
        "print(\"Training VGGNet Without Dropout on CIFAR-100\")\n",
        "train_losses_no_dropout_100, val_losses_no_dropout_100, val_acc_no_dropout_100 = train_model(VGGModified(num_classes=100, use_dropout=False), trainloader_100, testloader_100)\n",
        "\n",
        "print(\"Training VGGNet With Dropout on CIFAR-100\")\n",
        "train_losses_dropout_100, val_losses_dropout_100, val_acc_dropout_100 = train_model(VGGModified(num_classes=100, use_dropout=True), trainloader_100, testloader_100)\n",
        "\n",
        "\n",
        "######Result in Tableee\n",
        "results_df = pd.DataFrame({\n",
        "    \"Dataset\": [\"CIFAR-10\", \"CIFAR-10\", \"CIFAR-100\", \"CIFAR-100\"],\n",
        "    \"Dropout\": [\"No\", \"Yes\", \"No\", \"Yes\"],\n",
        "    \"Final Val Accuracy (%)\": [val_acc_no_dropout[-1], val_acc_dropout[-1], val_acc_no_dropout_100[-1], val_acc_dropout_100[-1]],\n",
        "    \"Final Val Loss\": [val_losses_no_dropout[-1], val_losses_dropout[-1], val_losses_no_dropout_100[-1], val_losses_dropout_100[-1]]\n",
        "})\n",
        "print(results_df)\n",
        "\n"
      ]
    }
  ]
}